#!/usr/bin/env python3

# Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0

import json
import argparse
import re
from datetime import datetime, timezone

import botocore_amazon.monkeypatch
import boto3
import time
import logging
from textwrap import TextWrapper


################################################################

def create_parser():
    arg = argparse.ArgumentParser(
        description='Scan CloudWatch logs for validation errors.')

    arg.add_argument('--profile',
                     metavar='PROFILE',
                     default='default',
                     help="""
                     The AWS account profile (default: %(default)s).
                     """
                    )
    arg.add_argument('--utc',
                     metavar='TIME',
                     required=True,
                     help="""
                     The time to begin the log search.  This is a UTC
                     time given as any valid ISO date string such as
                     YYYY-MM-DDTHH:MM:SS.
                     """
                    )
    arg.add_argument('--interval',
                     nargs='+',
                     metavar='M',
                     type=int,
                     default=[10, 60],
                     help="""
                     The interval about the start time to search the
                     logs.  Use --interval A to begin the search A
                     minutes before the time given by --utc. Use
                     --interval A B to begin the search A minutes
                     before --utc and end B minutes after --utc
                     (default: --interval 10 60).
                     """
                    )
    arg.add_argument('--errors',
                     action='store_true',
                     help="""
                     Display the names of the CBMC jobs generating validation
                     errors.
                     """
                    )
    arg.add_argument('--webhook',
                     metavar='JOB',
                     help="""
                     Display the logs for the webhook that launched
                     the CBMC Batch task named TASK
                     (TASK probably has the form PROOF-YYYYMMDD-HHMMSS).
                     """
                    )
    arg.add_argument('--batch',
                     metavar='TASK',
                     help="""
                     Display the logs for the CBMC Batch job named JOB
                     (JOB probably has the form PROOF-YYYYMMDD-HHMMSS-build).
                     """
                    )
    arg.add_argument('--dump',
                     metavar='LOG',
                     help="""
                     Display log records from log whose name contains
                     the string LOG (case insensitive).
                     """
                    )
    arg.add_argument('--brief',
                     action='store_true',
                     help="""
                     Display brief log messages.
                     """
                    )

    arg.add_argument('--batch-job-failures',
                     action='store_true',
                     help="""
                     Display the list of AWS Batch job failures.
                     """
                    )

    arg.add_argument('--correlation-ids',
                     action='store_true',
                     help="""
                     Display the list of correlation ids (describing a proof run) and their associated git commit information.
                     """
                     )

    arg.add_argument('--task-tree',
                     metavar='CORRELATION_ID',
                     action='store',
                     help="""
                     List all tasks associated with a correlation id, their relationships, and their status.  Use a correlation_id as an
                     argument.
                     """)

    arg.add_argument('--task-log',
                     action='store',
                     nargs=2,
                     help="""
                     Dump all log entries associated with the task. Requires the task name and the task id as arguments
                     """)

    arg.add_argument('--diagnose',
                     metavar='DIAGNOSE',
                     help="""
                     Diagnose failures using correlation id.  Use a correlation_id discovered by the --correlation_ids 
                     flag as an argument.
                     """
                     )

    arg.add_argument('--detail',
                     type=int,
                     default=1,
                     help="""
                         Level of detail to print for log information.
                         """
                     )
    return arg

################################################################

################################################################

class LogGroups:
    """Manage the log groups for AWS CloudWatch logs."""

    def __init__(self, session):
        self.client = session.client('logs')
        # Updating a cloudwatch stack can create a second instance
        # of a log group with a different hexadecimal suffix.
        # We restrict attention to the most recently created log group.
        self.log_groups = sorted(
            self.client.describe_log_groups()['logGroups'],
            key=lambda group: group['creationTime'],
            reverse=True
        )

    def log_group(self, name):
        """Log group whose name contains name as a substring."""

        log_groups = [group['logGroupName'] for group in self.log_groups
                      if name.lower() in group['logGroupName'].lower()]
        if not log_groups:
            logging.info("Failed to find log group with name %s", name)
            return None
        if len(log_groups) > 1:
            logging.info("Ignoring log groups with name %s: %s",
                         name, log_groups)
        log_name = log_groups[0]
        logging.info("Found log group with name %s: %s", name, log_name)
        return log_name

    def webhook(self):
        """Log group for the webhook lambda."""
        return self.log_group('github-HandleWebhookLambda')

    def invoke(self):
        """Log group for the batch invocation lambda."""
        return self.log_group('github-InvokeBatchLambda')

    def status(self):
        """Log group for the batch status lambda."""
        return self.log_group('github-BatchStatusLambda')

    def batch(self):
        """Log group for AWS Batch."""
        return self.log_group('/aws/batch/job')

    def prepare(self):
        """Log group for prepare source."""
        return self.log_group('prepare')

    def matching_streams(self, log_group, pattern=None,
                         start_timestamp=None, end_timestamp=None,
                         text=None):
        log_group = self.log_group(log_group)

        kwargs = {"logGroupName": log_group,
                  "startTime": start_timestamp,
                  "endTime": end_timestamp,
                  "filterPattern": pattern}
        paginator = self.client.get_paginator('filter_log_events')
        page_iterator = paginator.paginate(**kwargs)
        logging.info("LogGroups filter log events arguments: %s", kwargs)

        log_items = []
        text = text or 'Reading logs'
        print(text, end='', flush=True)
        for page in page_iterator:
            log_items.extend(page['events'])
            print(" .", end='', flush=True)
        print(" done (found {} items)".format(len(log_items)))
        logging.debug("LogGroups filter log events results: %s", log_items)

        log_streams = list({event['logStreamName'] for event in log_items})
        logging.info("Returning log group: %s", log_group)
        logging.info("Returning log streams: %s", log_streams)
        logging.info("Returning log items: %s", log_items)
        return log_group, log_streams, log_items

    def read_stream(self, log_group, log_stream):
        kwargs = {'logGroupName': log_group,
                  'logStreamName': log_stream,
                  'startFromHead': True
                  }
        events = self.client.get_log_events(**kwargs)
        return events

################################################################

class StatusLog:
    """Manage lambda logs for batch status"""

    def __init__(self, client, loggroupname, starttime, endtime=None):

        self.errors_ = []

        kwargs = {}
        kwargs['logGroupName'] = loggroupname
        kwargs['startTime'] = starttime
        if endtime:
            kwargs['endTime'] = endtime

        paginator = client.get_paginator('filter_log_events')
        page_iterator = paginator.paginate(** kwargs)

        events = []
        print("Reading log events...")
        for page in page_iterator:
            events.extend(page['events'])
            print("Reading log events...")
        events.sort(key=lambda event: event['timestamp'])

        error_found = False
        for event in events:
            msg = event['message'].rstrip()
            if msg.startswith('Unexpected Verification Result'):
                error_found = True
                continue
            if msg.startswith('Start: Updating GitHub status') and error_found:
                jobname = msg.split()[-2]
                jobtime = event['timestamp']
                self.errors_.append({'name': jobname, 'time': jobtime})
                error_found = False
                continue

        self.errors_.sort(key=lambda item: item['time'])

    def errors(self):
        return self.errors_

################################################################

class InvokeLog:
    """Manage lambda logs for webhooks."""

    def __init__(self, client, loggroupname, starttime, endtime=None, job=None):

        self.log_ = {}
        self.job_ = {}
        self.trigger_ = {}
        self.tarfile_ = {}

        kwargs = {}
        kwargs['logGroupName'] = loggroupname
        kwargs['startTime'] = starttime
        if endtime:
            kwargs['endTime'] = endtime

        paginator = client.get_paginator('filter_log_events')
        page_iterator = paginator.paginate(** kwargs)

        events = []
        print("Reading log events...")
        for page in page_iterator:
            events.extend(page['events'])
            print("Reading log events...")
        events.sort(key=lambda event: event['timestamp'])

        current_id = None
        active_id = False

        for event in events:
            msg = event['message'].rstrip()

            # Start of ci invocation lambda
            # Match "START RequestId: ID Version: $LATEST"
            if msg.startswith('START RequestId:'):
                current_id = msg.split()[2]
                self.log_[current_id] = []
                active_id = True

            # CI invocation aborted: uninterested pull request
            # Match "Ignoring pull request with action ..."
            # Match "Ignoring pull request action as base repository matches head"
            # Match "Ignoring delete-branch push event"
            if msg.startswith('Ignoring pull request'):
                self.log_[current_id].append('Ignoring pull request')
                active_id = False
            if msg.startswith('Ignoring delete-branch push event'):
                self.log_[current_id].append('Ignoring push')
                active_id = False

            # Report of cbmc batch job submission
            # Match "Launching job JOBNAME:"
            if msg.startswith('\nLaunching job'):
                jobname = msg.split()[2][:-1]
                self.job_[jobname] = current_id

            # Report of triggering action
            # Match "Pull request: {action} {from_repo} -> {to_repo}"
            # Match "Push to {}: {}"
            if msg.startswith('Pull request'):
                self.trigger_[current_id] = msg
            if msg.startswith('Push to'):
                self.trigger_[current_id] = msg

            # Report of tar file
            # Match "downloading https://api.github.com/.*/tarball/.* to .*
            if (msg.startswith('downloading https://api.github.com')
                    and msg.endswith('.tar.gz')):
                self.tarfile_[current_id] = msg.split()[-1].split('/')[-1]

            # End of ci invocation lambda
            # Match "END RequestId: ID"
            if msg.startswith('END RequestId:'):
                request_id = msg.split()[2]
                if current_id == request_id: # nested invocation report?
                    self.log_[current_id].append(msg)
                    active_id = False
                    # Stop the search if we've found the job we care about
                    if job and self.job_.get(job):
                        return

            if active_id:
                self.log_[current_id].append(msg)

    def invocation(self, job):
        return self.log_[self.job_[job]]

    def trigger(self, job):
        return self.trigger_[self.job_[job]]

    def tarfile(self, job):
        return self.tarfile_[self.job_[job]]

################################################################

class BatchLog:
    """Manage AWS Batch logs for CBMC Batch"""

    def __init__(self, client, loggroupname, starttime, endtime=None, task=None):

        self.boot_options_ = {}
        self.start_time_ = {}
        self.log_stream_ = {}
        self.log_ = {}

        kwargs = {}
        kwargs['logGroupName'] = loggroupname
        kwargs['startTime'] = starttime
        if endtime:
            kwargs['endTime'] = endtime

        paginator = client.get_paginator('filter_log_events')
        page_iterator = paginator.paginate(** kwargs)

        events = []
        print("Reading log events...")
        for page in page_iterator:
            events.extend(page['events'])
            print("Reading log events...")
        events.sort(key=lambda event: (event['logStreamName'], event['timestamp']))

        task_name = None
        task_found = False
        for event in events:
            for msg in event['message'].rstrip().split('\r'):
                if msg.startswith('Booting with options'):
                    # Starting the scan of the next AWS Batch job, and
                    # ending the scan of the prior job.

                    # Stop if the prior job was the job we were looking for.
                    if task_found:
                        return

                    boot_json = msg[len('Booting with options'):]
                    boot_options = json.loads(boot_json)
                    task_name = boot_options['jobname']
                    task_found = task == task_name

                    self.boot_options_[task_name] = boot_options
                    self.start_time_[task_name] = event['timestamp']
                    self.log_stream_[task_name] = event['logStreamName']
                    self.log_[task_name] = []

                if task_name:
                    self.log_[task_name].append(msg)


    def boot_options(self, task_name):
        return self.boot_options_[task_name]

    def start_time(self, task_name):
        return self.start_time_[task_name]

    def log_stream(self, task_name):
        return self.log_stream_[task_name]

    def log(self, task_name):
        return self.log_[task_name]

################################################################

def time_from_iso(timeiso):
    if timeiso is None:
        return None
    lcltime = datetime.fromisoformat(timeiso)
    gmttime = lcltime.replace(tzinfo=timezone.utc)
    return int(gmttime.timestamp() * 1000)

def iso_from_time(timems):
    if timems is None:
        return None
    return datetime.utcfromtimestamp(timems // 1000).isoformat()

################################################################

def dump_log(client, group, utcstart, utcend, logname, brief=False):
    logname = group.log_name(logname)
    if logname:
        kwargs = {}
        kwargs['logGroupName'] = logname
        kwargs['startTime'] = utcstart
        if utcend:
            kwargs['endTime'] = utcend

        paginator = client.get_paginator('filter_log_events')
        page_iterator = paginator.paginate(** kwargs)

        events = []
        print("Reading log events...")
        for page in page_iterator:
            events.extend(page['events'])
            print("Reading log events...")
        events.sort(key=lambda event: event['timestamp'])
        for event in events:
            for msg in event['message'].rstrip().split('\r'):
                if brief:
                    msg = msg[:80]
                print(iso_from_time(event['timestamp']), msg)

def dump_batch_status(session, status='FAILED', brief=False):
        kwargs = {}
        kwargs['jobQueue'] = 'CBMCJobQueue'
        kwargs['jobStatus'] = 'FAILED'

        client = session.client('batch')
        paginator = client.get_paginator('list_jobs')
        from pprint import pprint
        pprint(paginator)
        pprint(kwargs)
        page_iterator = paginator.paginate(** kwargs)

        events = []
        print("Reading log events...")
        for page in page_iterator:
            events.extend(page['jobSummaryList'])
            print("Reading log events...")

        events.sort(key=lambda event: event['createdAt'])
        job_failed = []
        task_exited = []
        for event in events:
            try:
                jobname = event['jobName']
                if event.get('container') and event['container'].get('reason'):
                    error = event['container']['reason']
                else:
                    error = event['statusReason']
                if error == 'Dependent Job failed':
                    job_failed.append(jobname)
                    continue
                if error == 'Essential container in task exited':
                    task_exited.append(jobname)
                    continue
                if brief:
                    error = error[:80]
                timestamp = iso_from_time(event['createdAt'])
            except Exception as e:
                print('error')
                pprint(event)
                raise e
            print(timestamp, jobname, error)
        if job_failed:
            print('Dependent Job failed')
            for job in job_failed:
                print('  '+job)
        if task_exited:
            print('Essential container in task exited')
            for job in task_exited:
                print('  '+job)

###############################################################
# From Mark's proof.py file
###############################################################

class WebHook:
    """Parse the webhook payload."""

    def __init__(self, payload):

        webhook = json.loads(payload)
        headers = {k.lower(): v for k, v in webhook["headers"].items()}
        body = json.loads(webhook["body"])

        self.event_type = headers.get('x-github-event')
        # The repository being written to
        self.base_name = None
        self.base_branch = None
        self.base_sha = None
        # The repository being read from
        self.head_name = None
        self.head_branch = None
        self.head_sha = None
        # A url describing the event
        self.url = None

        if self.event_type == 'pull_request':
            self.base_name = body["pull_request"]["base"]["repo"]["full_name"]
            self.base_branch = body["pull_request"]["base"]["ref"]
            self.base_sha = body["pull_request"]["base"]["sha"]
            self.head_name = body["pull_request"]["head"]["repo"]["full_name"]
            self.head_branch = body["pull_request"]["head"]["ref"]
            self.head_sha = body["pull_request"]["head"]["sha"]
            self.url = body["pull_request"]["html_url"]
            return

        if self.event_type == 'push':
            self.base_name = body["repository"]["full_name"]
            self.base_branch = body["ref"]
            head_commit = None
            if body.get('head_commit'):
                head_commit = body['head_commit']
            elif body.get('commits'):
                head_commit = body['commits'][-1]
            elif body.get('after'):
                head_commit = {'id': body['after']}
            if head_commit:
                self.head_sha = head_commit.get('id')
                self.url = head_commit.get('url')
            return

        raise UserWarning('Unknown event type: {}'.format(self.event_type))

    def summary(self, detail=1):
        result = {
            'event_type': self.event_type,
            'base_name': self.base_name,
            'base_branch': self.base_branch,
            'base_sha': self.base_sha,
            'head_name': self.head_name,
            'head_branch': self.head_branch,
            'head_sha': self.head_sha,
            'url': self.url
        }
        return result

################################################################

################################################################
# MWW additions
################################################################

def await_query_result(client, query_id):
    kwargs = {'queryId': query_id}
    result = client.get_query_results(**kwargs)
    while result['status'] in set(['Scheduled', 'Running']):
        print("Query result = {}.  Waiting 2 seconds".format(result['status']))
        time.sleep(2)
        result = client.get_query_results(**kwargs)
    return result


def query_result_to_list_dict(result):
    list_dict = []
    for log_event in result['results']:
        event_dict = {}
        for kvp in log_event:
            event_dict[kvp["field"]] = kvp["value"]
        list_dict.append(event_dict)
    return list_dict


def start_query(client, loggroupnames, query, starttime, endtime):
    print("limiting query to the first 1000 results")
    kwargs = {'logGroupNames': loggroupnames,
              'startTime': starttime,
              'queryString': query,
              'limit': 1000}
    if endtime:
        kwargs['endTime'] = endtime

    # start the query
    # print("start_query args: " + str(kwargs))
    result = client.start_query(**kwargs)
    return result['queryId']


def dump_correlation_ids(client, group, start_time, end_time):
    log_groups = [group.webhook()]
    query = ("fields correlation_list.0, @timestamp, event.body "
             "| filter ispresent(correlation_list.0) "
             "| filter task_name = \"HandleWebhookLambda\" "
             "| filter status like /COMPLETED/")
    query_id = start_query(client, log_groups, query, start_time, end_time)
    result = await_query_result(client, query_id)
    list_dict = query_result_to_list_dict(result)

    textwrap = TextWrapper(subsequent_indent=set_indent(33), width=120)
    print("{0:<40} {1:<23} {2}".format("correlation_list", "timestamp", "Github commit information"))

    for elem in list_dict:
        print(textwrap.fill("{0:<40} {1:<23} {2}".format(elem["correlation_list.0"], elem["@timestamp"], str(elem.get("event.body")))))


### Task tree related stuff.
class KeyTree():
    def __init__(self, key, elements = None):
        self.key = key
        self.children = {}
        self.elements = []

    def mk_child(self, head):
        return KeyTree(head)

    def add_element(self, key_path, element):
        if not key_path:
            self.elements.append(element)
        else:
            head, *tail = key_path
            if not (self.children.get(head)):
                self.children[head] = self.mk_child(head)
            self.children[head].add_element(tail, element)

def set_indent(depth):
    return "" if depth == 0 else "  " + set_indent(depth-1)

class TaskTree(KeyTree):

    def mk_child(self, head):
        return TaskTree(head)

    def launch_msgs(self):
        return filter(lambda x: x['status'].startswith("LAUNCH"), self.elements)

    def started_msgs(self):
        return filter(lambda x: x['status'].startswith("STARTED"), self.elements)

    def completed_msgs(self):
        return filter(lambda x: x['status'].startswith("COMPLETED"), self.elements)

    def timestamp_sorted_msgs(self):
        return sorted(self.elements, key=lambda x: x['@timestamp'])

def pprint_tasktree(root, detail, textwrapper = TextWrapper(width=120), depth=0):
    textwrapper.initial_indent = set_indent(depth)
    textwrapper.subsequent_indent = set_indent(depth + 1)

    if detail == 1:
        sorted_msgs = root.timestamp_sorted_msgs()
        if (sorted_msgs):
            elem = list(root.timestamp_sorted_msgs())[-1]
            summary = {"task_id": elem['task_id'], "task_name": elem['task_name'], "status": elem['status']}
            print(textwrapper.fill(str(summary)))
    else:
        print(textwrapper.fill("Key: " + str(root.key)))
        print(textwrapper.fill("Messages:"))
        for elem in root.elements:
            summary = {"task_id": elem['task_id'], "task_name": elem['task_name'], "status": elem['status']}
            print(textwrapper.fill(str(summary)))
            if detail > 2:
                print("Message body: " + str(elem))
    if (root.children):
        print(textwrapper.fill("Children:"))
    for child in root.children.values():
        pprint_tasktree(child, detail, textwrapper, depth + 1)


def dict_update_list(dict, key, val):
    dict[key] = dict.get(key, []).append(val)


def create_task_tree(client, group, correlation_id, start_time, end_time):
    log_groups = [group.webhook(), group.invoke(), group.status(), group.prepare()]
    query = "fields @timestamp, @message | filter correlation_list.0 = \"{}\" | filter ispresent(status)".format(
        correlation_id)
    # print("Query: " + query)
    query_id = start_query(client, log_groups, query, start_time, end_time)
    result = await_query_result(client, query_id)
    print(json.dumps(result))
    # print("Result: " + str(result))
    list_dict = query_result_to_list_dict(result)
    if not list_dict:
        print("No data for correlation_id {} during the specified interval".format(correlation_id))
        return []

    # print("list_dict: " + str(list_dict))
    # if we sort the keys in the dict, we have a depth-first view of the tree.
    task_tree = TaskTree("root", [])
    for elem in list_dict:
        msg = json.loads(elem['@message'])
        ts = elem['@timestamp']
        task_tree.add_element(msg['correlation_list'],
                              (ts, msg))
    return task_tree


def generate_sorted_message_list(client, kwargs, brief=False):
    print("kwargs: " + str(kwargs))
    paginator = client.get_paginator('filter_log_events')
    page_iterator = paginator.paginate(**kwargs)
    events = []
    print("Reading log events...")
    for page in page_iterator:
        events.extend(page['events'])
        print("Reading log events...")
    events.sort(key=lambda event: event['timestamp'])
    messages = []
    for event in events:
        ts = event['timestamp']
        messages.append((ts, event['message']))
    return messages


def dump_lambda_log(client, utcstart, utcend, logname, requestId):
    if logname:
        kwargs = {}
        kwargs['logGroupName'] = logname
        kwargs['startTime'] = utcstart
        if utcend:
            kwargs['endTime'] = utcend
        in_lambda = False
        messages = generate_sorted_message_list(client, kwargs)
        for (timestamp, msg) in messages:
            if msg.startswith('START RequestId: {}'.format(requestId)):
                in_lambda = True
            if in_lambda:
                print(iso_from_time(timestamp), msg)
            if msg.startswith('END RequestId: {}'.format(requestId)):
                in_lambda = False
    else:
        print("ERROR: log name not understood")

def dump_logstream(client, logname, logstream):
    if logname:
        kwargs = {}
        kwargs['logGroupName'] = logname
        kwargs['logStreamNames'] = [logstream]
        print("kwargs: " + str(kwargs))
        messages = generate_sorted_message_list(client, kwargs)
        for (timestamp, msg) in messages:
            print(iso_from_time(timestamp), msg)
    else:
        print("ERROR: log name not understood")


def dump_task_log(session, client, group, utcstart, utcend, task_name, task_id):
    if task_name == "cbmc_ci_start:lambda_handler":
        dump_lambda_log(client, utcstart, utcend, group.invoke(), task_id)
    elif task_name == "prepare_source:generate_cbmc_jobs":
        stream_id = task_id.split(":")[1]
        dump_logstream(client, group.prepare(), stream_id)
    elif task_name == "HandleWebhookLambda":
        dump_lambda_log(client, utcstart, utcend, group.webhook(), task_id)
    else:
        job_name_pattern = r"([\S]+)"
        timestamp_pattern = r"\d{8}-\d{6}"
        pattern = job_name_pattern + timestamp_pattern + "-([a-z]*)$"
        res = re.search(pattern, task_name)
        if res:
            batch = session.client('batch')
            result = batch.describe_jobs(jobs = [task_id])
            jobs = result['jobs']
            if not jobs:
                print("ERROR: task_id {} does not have an associated log.  AWS Batch logs are disposed after 24 hours".format(task_id))
            else:
                log_stream = jobs[0]['container']['logStreamName']
                dump_logstream(client, group.batch(), log_stream)
        else:
            print("ERROR: task_name {} does not match expected task formats".format(task_name))

    ################################################################

def scan(args, session, client):
    utctime = time_from_iso(args.utc)
    start = args.interval[0]
    try:
        end = args.interval[1]
    except IndexError:
        end = None
    utcstart = utctime - (start * 60 * 1000)
    utcend = utctime + (end * 60 * 1000) if end else None

    group = LogGroups(client)

    if args.errors:
        status = StatusLog(client, group.status(), utcstart, utcend)
        for stat in status.errors():
            print("{} ({})".format(stat['name'], iso_from_time(stat['time'])))

    if args.webhook:
        invoke = InvokeLog(client, group.invoke(), utcstart, utcend, args.webhook)
        print()
        print("Trigger: {}".format(invoke.trigger(args.webhook)))
        print("Tarfile: {}".format(invoke.tarfile(args.webhook)))
        print("Log records: displayed below")
        print()
        for line in invoke.invocation(args.webhook):
            print(line)

    if args.batch:
        invoke = BatchLog(client, group.batch(), utcstart, utcend, args.batch)
        print()
        print("CBMC Task: {}".format(args.batch))
        print("Start time: {}".format(iso_from_time(invoke.start_time(args.batch))))
        print("Log stream name: {}".format(invoke.log_stream(args.batch)))
        print(json.dumps(invoke.boot_options(args.batch), indent=2))
        print()
        for line in invoke.log(args.batch):
            print(line)

    if args.dump:
        dump_log(client, group, utcstart, utcend, args.dump, args.brief)

    if args.correlation_ids:
        dump_correlation_ids(client, group, utcstart, utcend)

    if args.task_tree:
        create_task_tree(client, group, args.task_tree, utcstart, utcend)

    if args.task_log:
        print("task_log args: " + str(args.task_log))
        dump_task_log(session, client, group, utcstart, utcend, args.task_log[0], args.task_log[1])

def temp(session):
    awsbatch = session.client('batch')
    results = awsbatch.describe_job_queues()
    print("describe job queues: " + str(results))
    print("Attempting listitems for queue cbmc")
    results = awsbatch.list_jobs(jobQueue='CBMCJobQueue',
                                 jobStatus= "FAILED")
    print(str(results))

def main():
    args = create_parser().parse_args()
    session = boto3.session.Session(profile_name=args.profile)

    if args.batch_job_failures:
        dump_batch_status(session, brief=args.brief)
        return

    client = session.client('logs')
    scan(args, session, client)

    # temp(session)

if __name__ == '__main__':
    main()
