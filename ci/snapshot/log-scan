#!/usr/bin/env python3

# Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0

import json
import argparse
from datetime import datetime, timezone

import botocore_amazon.monkeypatch
import boto3
import time
from textwrap import TextWrapper


################################################################

def create_parser():
    arg = argparse.ArgumentParser(
        description='Scan CloudWatch logs for validation errors.')

    arg.add_argument('--profile',
                     metavar='PROFILE',
                     default='default',
                     help="""
                     The AWS account profile (default: %(default)s).
                     """
                    )
    arg.add_argument('--utc',
                     metavar='TIME',
                     required=True,
                     help="""
                     The time to begin the log search.  This is a UTC
                     time given as any valid ISO date string such as
                     YYYY-MM-DDTHH:MM:SS.
                     """
                    )
    arg.add_argument('--interval',
                     nargs='+',
                     metavar='M',
                     type=int,
                     default=[10, 60],
                     help="""
                     The interval about the start time to search the
                     logs.  Use --interval A to begin the search A
                     minutes before the time given by --utc. Use
                     --interval A B to begin the search A minutes
                     before --utc and end B minutes after --utc
                     (default: --interval 10 60).
                     """
                    )
    arg.add_argument('--errors',
                     action='store_true',
                     help="""
                     Display the names of the CBMC jobs generating validation
                     errors.
                     """
                    )
    arg.add_argument('--webhook',
                     metavar='JOB',
                     help="""
                     Display the logs for the webhook that launched
                     the CBMC Batch task named TASK
                     (TASK probably has the form PROOF-YYYYMMDD-HHMMSS).
                     """
                    )
    arg.add_argument('--batch',
                     metavar='TASK',
                     help="""
                     Display the logs for the CBMC Batch job named JOB
                     (JOB probably has the form PROOF-YYYYMMDD-HHMMSS-build).
                     """
                    )
    arg.add_argument('--dump',
                     metavar='LOG',
                     help="""
                     Display log records from log whose name contains
                     the string LOG (case insensitive).
                     """
                    )
    arg.add_argument('--brief',
                     action='store_true',
                     help="""
                     Display brief log messages.
                     """
                    )
    arg.add_argument('--batch-job-failures',
                     action='store_true',
                     help="""
                     Display the list of AWS Batch job failures.
                     """
                    )

    arg.add_argument('--correlation-ids',
                     action='store_true',
                     help="""
                     Display the list of correlation ids (describing a proof run) and their associated git commit information.
                     """
                     )

    arg.add_argument('--task-tree',
                     metavar='CORRELATION_ID',
                     action='store',
                     help="""
                     List all tasks associated with a correlation id, their relationships, and their status.  Use a correlation_id as an
                     argument.
                     """)

    arg.add_argument('--task-log',
                     action='store_true',
                     help="""
                     Dump the log associated with the task""")

    arg.add_argument('--diagnose',
                     metavar='DIAGNOSE',
                     help="""
                     Diagnose failures using correlation id.  Use a correlation_id discovered by the --correlation_ids 
                     flag as an argument.
                     """
                     )


    return arg

################################################################

class LogGroup:
    """Manage AWS CloudWatch log group names"""

    def __init__(self, client):

        self.webhook_ = {'name': None, 'time': 0}
        self.invoke_ = {'name': None, 'time': 0}
        self.batch_ = {'name': None, 'time': 0}
        self.status_ = {'name': None, 'time': 0}
        self.prepare_ = {'name': None, 'time': 0}

        self.logs = client.describe_log_groups()['logGroups']

        for desc in self.logs:
            name = desc['logGroupName']
            start = desc['creationTime']
            if name.find('github-HandleWebhookLambda') > -1:
                if self.webhook_['time'] < start:
                    self.webhook_ = {'name': name, 'time': start}
                continue
            if name.find('github-InvokeBatchLambda') > -1:
                if self.invoke_['time'] < start:
                    self.invoke_ = {'name': name, 'time': start}
                continue
            if name.find('github-BatchStatusLambda') > -1:
                if self.status_['time'] < start:
                    self.status_ = {'name': name, 'time': start}
                continue
            if name.find('/aws/batch/job') > -1:
                if self.batch_['time'] < start:
                    self.batch_ = {'name': name, 'time': start}
                continue
            if name.find('/aws/codebuild/Prepare-Source-Project') > -1:
                if self.prepare_['time'] < start:
                    self.prepare_ = {'name': name, 'time': start}
                continue

    def webhook(self):
        """Log group for the webhook lambda."""
        return self.webhook_['name']

    def invoke(self):
        """Log group for the batch invocation lambda."""
        return self.invoke_['name']

    def status(self):
        """Log group for the batch status lambda."""
        return self.status_['name']

    def batch(self):
        """Log group for AWS Batch."""
        return self.batch_['name']

    def prepare(self):
        """Log group for prepare source project container"""
        return self.prepare_['name']

    def log_name(self, log):
        """Log group for log containing the string log."""
        for desc in self.logs:
            name = desc['logGroupName']
            if name.lower().find(log.lower()) > -1:
                return name
        return None

################################################################

class StatusLog:
    """Manage lambda logs for batch status"""

    def __init__(self, client, loggroupname, starttime, endtime=None):

        self.errors_ = []

        kwargs = {}
        kwargs['logGroupName'] = loggroupname
        kwargs['startTime'] = starttime
        if endtime:
            kwargs['endTime'] = endtime

        paginator = client.get_paginator('filter_log_events')
        page_iterator = paginator.paginate(** kwargs)

        events = []
        print("Reading log events...")
        for page in page_iterator:
            events.extend(page['events'])
            print("Reading log events...")
        events.sort(key=lambda event: event['timestamp'])

        error_found = False
        for event in events:
            msg = event['message'].rstrip()
            if msg.startswith('Unexpected Verification Result'):
                error_found = True
                continue
            if msg.startswith('Start: Updating GitHub status') and error_found:
                jobname = msg.split()[-2]
                jobtime = event['timestamp']
                self.errors_.append({'name': jobname, 'time': jobtime})
                error_found = False
                continue

        self.errors_.sort(key=lambda item: item['time'])

    def errors(self):
        return self.errors_

################################################################

class InvokeLog:
    """Manage lambda logs for webhooks."""

    def __init__(self, client, loggroupname, starttime, endtime=None, job=None):

        self.log_ = {}
        self.job_ = {}
        self.trigger_ = {}
        self.tarfile_ = {}

        kwargs = {}
        kwargs['logGroupName'] = loggroupname
        kwargs['startTime'] = starttime
        if endtime:
            kwargs['endTime'] = endtime

        paginator = client.get_paginator('filter_log_events')
        page_iterator = paginator.paginate(** kwargs)

        events = []
        print("Reading log events...")
        for page in page_iterator:
            events.extend(page['events'])
            print("Reading log events...")
        events.sort(key=lambda event: event['timestamp'])

        current_id = None
        active_id = False

        for event in events:
            msg = event['message'].rstrip()

            # Start of ci invocation lambda
            # Match "START RequestId: ID Version: $LATEST"
            if msg.startswith('START RequestId:'):
                current_id = msg.split()[2]
                self.log_[current_id] = []
                active_id = True

            # CI invocation aborted: uninterested pull request
            # Match "Ignoring pull request with action ..."
            # Match "Ignoring pull request action as base repository matches head"
            # Match "Ignoring delete-branch push event"
            if msg.startswith('Ignoring pull request'):
                self.log_[current_id].append('Ignoring pull request')
                active_id = False
            if msg.startswith('Ignoring delete-branch push event'):
                self.log_[current_id].append('Ignoring push')
                active_id = False

            # Report of cbmc batch job submission
            # Match "Launching job JOBNAME:"
            if msg.startswith('\nLaunching job'):
                jobname = msg.split()[2][:-1]
                self.job_[jobname] = current_id

            # Report of triggering action
            # Match "Pull request: {action} {from_repo} -> {to_repo}"
            # Match "Push to {}: {}"
            if msg.startswith('Pull request'):
                self.trigger_[current_id] = msg
            if msg.startswith('Push to'):
                self.trigger_[current_id] = msg

            # Report of tar file
            # Match "downloading https://api.github.com/.*/tarball/.* to .*
            if (msg.startswith('downloading https://api.github.com')
                    and msg.endswith('.tar.gz')):
                self.tarfile_[current_id] = msg.split()[-1].split('/')[-1]

            # End of ci invocation lambda
            # Match "END RequestId: ID"
            if msg.startswith('END RequestId:'):
                request_id = msg.split()[2]
                if current_id == request_id: # nested invocation report?
                    self.log_[current_id].append(msg)
                    active_id = False
                    # Stop the search if we've found the job we care about
                    if job and self.job_.get(job):
                        return

            if active_id:
                self.log_[current_id].append(msg)

    def invocation(self, job):
        return self.log_[self.job_[job]]

    def trigger(self, job):
        return self.trigger_[self.job_[job]]

    def tarfile(self, job):
        return self.tarfile_[self.job_[job]]

################################################################

class BatchLog:
    """Manage AWS Batch logs for CBMC Batch"""

    def __init__(self, client, loggroupname, starttime, endtime=None, task=None):

        self.boot_options_ = {}
        self.start_time_ = {}
        self.log_stream_ = {}
        self.log_ = {}

        kwargs = {}
        kwargs['logGroupName'] = loggroupname
        kwargs['startTime'] = starttime
        if endtime:
            kwargs['endTime'] = endtime

        paginator = client.get_paginator('filter_log_events')
        page_iterator = paginator.paginate(** kwargs)

        events = []
        print("Reading log events...")
        for page in page_iterator:
            events.extend(page['events'])
            print("Reading log events...")
        events.sort(key=lambda event: (event['logStreamName'], event['timestamp']))

        task_name = None
        task_found = False
        for event in events:
            for msg in event['message'].rstrip().split('\r'):
                if msg.startswith('Booting with options'):
                    # Starting the scan of the next AWS Batch job, and
                    # ending the scan of the prior job.

                    # Stop if the prior job was the job we were looking for.
                    if task_found:
                        return

                    boot_json = msg[len('Booting with options'):]
                    boot_options = json.loads(boot_json)
                    task_name = boot_options['jobname']
                    task_found = task == task_name

                    self.boot_options_[task_name] = boot_options
                    self.start_time_[task_name] = event['timestamp']
                    self.log_stream_[task_name] = event['logStreamName']
                    self.log_[task_name] = []

                if task_name:
                    self.log_[task_name].append(msg)


    def boot_options(self, task_name):
        return self.boot_options_[task_name]

    def start_time(self, task_name):
        return self.start_time_[task_name]

    def log_stream(self, task_name):
        return self.log_stream_[task_name]

    def log(self, task_name):
        return self.log_[task_name]

################################################################

def time_from_iso(timeiso):
    if timeiso is None:
        return None
    lcltime = datetime.fromisoformat(timeiso)
    gmttime = lcltime.replace(tzinfo=timezone.utc)
    return int(gmttime.timestamp() * 1000)

def iso_from_time(timems):
    if timems is None:
        return None
    return datetime.utcfromtimestamp(timems // 1000).isoformat()

################################################################

def dump_log(client, group, utcstart, utcend, logname, brief=False):
    logname = group.log_name(logname)
    if logname:
        kwargs = {}
        kwargs['logGroupName'] = logname
        kwargs['startTime'] = utcstart
        if utcend:
            kwargs['endTime'] = utcend

        paginator = client.get_paginator('filter_log_events')
        page_iterator = paginator.paginate(** kwargs)

        events = []
        print("Reading log events...")
        for page in page_iterator:
            events.extend(page['events'])
            print("Reading log events...")
        events.sort(key=lambda event: event['timestamp'])
        for event in events:
            for msg in event['message'].rstrip().split('\r'):
                if brief:
                    msg = msg[:80]
                print(iso_from_time(event['timestamp']), msg)

def dump_batch_status(session, status='FAILED', brief=False):
        kwargs = {}
        kwargs['jobQueue'] = 'CBMCJobQueue'
        kwargs['jobStatus'] = 'FAILED'

        client = session.client('batch')
        paginator = client.get_paginator('list_jobs')
        from pprint import pprint
        pprint(paginator)
        pprint(kwargs)
        page_iterator = paginator.paginate(** kwargs)

        events = []
        print("Reading log events...")
        for page in page_iterator:
            events.extend(page['jobSummaryList'])
            print("Reading log events...")

        events.sort(key=lambda event: event['createdAt'])
        job_failed = []
        task_exited = []
        for event in events:
            try:
                jobname = event['jobName']
                if event.get('container') and event['container'].get('reason'):
                    error = event['container']['reason']
                else:
                    error = event['statusReason']
                if error == 'Dependent Job failed':
                    job_failed.append(jobname)
                    continue
                if error == 'Essential container in task exited':
                    task_exited.append(jobname)
                    continue
                if brief:
                    error = error[:80]
                timestamp = iso_from_time(event['createdAt'])
            except Exception as e:
                print('error')
                pprint(event)
                raise e
            print(timestamp, jobname, error)
        if job_failed:
            print('Dependent Job failed')
            for job in job_failed:
                print('  '+job)
        if task_exited:
            print('Essential container in task exited')
            for job in task_exited:
                print('  '+job)

################################################################
# MWW additions
################################################################

def await_query_result(client, query_id):
    kwargs = {'queryId': query_id}
    result = client.get_query_results(**kwargs)
    while result['status'] in set(['Scheduled', 'Running']):
        print("Query result = {}.  Waiting 2 seconds".format(result['status']))
        time.sleep(2)
        result = client.get_query_results(**kwargs)
    return result


def query_result_to_list_dict(result):
    list_dict = []
    for log_event in result['results']:
        event_dict = {}
        for kvp in log_event:
            event_dict[kvp["field"]] = kvp["value"]
        list_dict.append(event_dict)
    return list_dict


def start_query(client, loggroupnames, query, starttime, endtime):
    print("limiting query to the first 1000 results")
    kwargs = {'logGroupNames': loggroupnames,
              'startTime': starttime,
              'queryString': query,
              'limit': 1000}
    if endtime:
        kwargs['endTime'] = endtime

    # start the query
    # print("start_query args: " + str(kwargs))
    result = client.start_query(**kwargs)
    return result['queryId']


def dump_correlation_ids(client, group, start_time, end_time):
    log_groups = [group.webhook()]
    query = ("fields correlation_list.0, event.body "
             "| filter ispresent(correlation_list.0) "
             "| filter task_name = \"HandleWebhookLambda\" "
             "| filter status like /COMPLETED/")
    query_id = start_query(client, log_groups, query, start_time, end_time)
    result = await_query_result(client, query_id)
    list_dict = query_result_to_list_dict(result)

    textwrap = TextWrapper(subsequent_indent=set_indent(21), width=120)
    print("{0:<40} {1}".format("correlation_list", "Github commit information"))

    for elem in list_dict:
        print(textwrap.fill("{0:<40} {1}".format(elem["correlation_list.0"], str(elem.get("event.body")))))


### Task tree related stuff.
class KeyTree():
    def __init__(self, key, elements = None):
        self.key = key
        self.children = {}
        self.elements = []

    def mk_child(self, head):
        return KeyTree(head)

    def add_element(self, key_path, element):
        if not key_path:
            self.elements.append(element)
        else:
            head, *tail = key_path
            if not (self.children.get(head)):
                self.children[head] = self.mk_child(head)
            self.children[head].add_element(tail, element)

def set_indent(depth):
    return "" if depth == 0 else "  " + set_indent(depth-1)

class TaskTree(KeyTree):

    def mk_child(self, head):
        return TaskTree(head)

    def launch_msgs(self):
        return filter(lambda x: x['status'].startswith("LAUNCH"), self.elements)

    def started_msgs(self):
        return filter(lambda x: x['status'].startswith("STARTED"), self.elements)

    def completed_msgs(self):
        return filter(lambda x: x['status'].startswith("COMPLETED"), self.elements)

    def timestamp_sorted_msgs(self):
        return sorted(lambda x: x['@timestamp'], self.elements)

    def pprint(self, textwrapper, depth):
        textwrapper.initial_indent = set_indent(depth)
        textwrapper.subsequent_indent = set_indent(depth+1)
        print(textwrapper.fill("Key: " + str(self.key)))
        print(textwrapper.fill("Messages:"))
        for elem in self.elements:
            summary = {"task_id" : elem['task_id'], "task_name" : elem['task_name'], "status" : elem['status']}
            print(textwrapper.fill(str(summary)))
        print(textwrapper.fill("Children:"))
        for child in self.children.values():
            child.pprint(textwrapper, depth+1)


def dict_update_list(dict, key, val):
    dict[key] = dict.get(key, []).append(val)

def create_task_tree(client, group, correlation_id, start_time, end_time):
    log_groups = [group.webhook(), group.invoke(), group.status(), group.prepare()]
    query = "fields @message | filter correlation_list.0 = \"{}\" | filter ispresent(status)".format(
        correlation_id)
    # print("Query: " + query)
    query_id = start_query(client, log_groups, query, start_time, end_time)
    result = await_query_result(client, query_id)
    # print("Result: " + str(result))
    list_dict = query_result_to_list_dict(result)
    if not list_dict:
        print("No data for correlation_id {} during the specified interval".format(correlation_id))
        return []

    # print("list_dict: " + str(list_dict))
    # if we sort the keys in the dict, we have a depth-first view of the tree.
    task_tree = TaskTree("root", [])
    for elem in list_dict:
        msg = json.loads(elem['@message'])
        task_tree.add_element(msg['correlation_list'],
                             msg)

    textwrapper = TextWrapper(width=120)
    task_tree.pprint(textwrapper, 0)

################################################################

def scan(args, client):
    utctime = time_from_iso(args.utc)
    start = args.interval[0]
    try:
        end = args.interval[1]
    except IndexError:
        end = None
    utcstart = utctime - (start * 60 * 1000)
    utcend = utctime + (end * 60 * 1000) if end else None

    group = LogGroup(client)

    if args.errors:
        status = StatusLog(client, group.status(), utcstart, utcend)
        for stat in status.errors():
            print("{} ({})".format(stat['name'], iso_from_time(stat['time'])))

    if args.webhook:
        invoke = InvokeLog(client, group.invoke(), utcstart, utcend, args.webhook)
        print()
        print("Trigger: {}".format(invoke.trigger(args.webhook)))
        print("Tarfile: {}".format(invoke.tarfile(args.webhook)))
        print("Log records: displayed below")
        print()
        for line in invoke.invocation(args.webhook):
            print(line)

    if args.batch:
        invoke = BatchLog(client, group.batch(), utcstart, utcend, args.batch)
        print()
        print("CBMC Task: {}".format(args.batch))
        print("Start time: {}".format(iso_from_time(invoke.start_time(args.batch))))
        print("Log stream name: {}".format(invoke.log_stream(args.batch)))
        print(json.dumps(invoke.boot_options(args.batch), indent=2))
        print()
        for line in invoke.log(args.batch):
            print(line)

    if args.dump:
        dump_log(client, group, utcstart, utcend, args.dump, args.brief)

    if args.correlation_ids:
        dump_correlation_ids(client, group, utcstart, utcend)

    if args.task_tree:
        create_task_tree(client, group, args.task_tree, utcstart, utcend)

    if args.diagnose:
        diagnose(client, group, args.diagnose, utcstart, utcend)

def temp(session):
    awsbatch = session.client('batch')
    results = awsbatch.describe_job_queues()
    print("describe job queues: " + str(results))
    print("Attempting listitems for queue cbmc")
    results = awsbatch.list_jobs(jobQueue='CBMCJobQueue',
                                 jobStatus= "FAILED")
    print(str(results))

def main():
    args = create_parser().parse_args()
    session = boto3.session.Session(profile_name=args.profile)

    if args.batch_job_failures:
        dump_batch_status(session, brief=args.brief)
        return

    client = session.client('logs')
    scan(args, client)

    # temp(session)

if __name__ == '__main__':
    main()
